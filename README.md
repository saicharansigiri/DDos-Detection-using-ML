# DDos-Detection-using-ML

* DDos detection using ML Techniques we have Trained our models using NSDL -Dataset & implemented 8 differenet ML Models To Detect DDos.
* We split our gathered data into training samples and testing samples.
* We used Jupyter Notebook for Training and Testing of our machine Learning Models.
* We evaluate both supervised learning and unsupervised learning algorithms. 
* For supervised classification,we evaluated the Following.
   - Linear Regression (LR),
   - SVM (with linear, RBF(Radial Basis Function) and polynomial kernels),Decision Tree,
   - Naive Bayes and Random Forest algorithms.
   - We also tested unsupervised learning algorithm i.e k-means.


 


### DDOS Report by imperva:- 
![image](https://user-images.githubusercontent.com/69844239/127859906-4638bbf5-dd5a-4521-a0f0-e2ab06f30478.png)
### Most Common Types of DDosAttacks:- 
![image](https://user-images.githubusercontent.com/69844239/127860077-fc633342-8ddf-44ad-86d7-4bb1f7cd12db.png)
### Proposed Model:- 
![image](https://user-images.githubusercontent.com/69844239/127859722-42963e97-145c-42ac-9647-c471b7c7025f.png)
### **Implementation And Evaluation**:-
* **Accuracy** : Accuracy is the fraction of correctly classified samples. This is the most commonly used metric to evaluate a model. The higher this value is, indicates the model is better.
                                    
                                    Accuracy = (TP+TN)/(TP+TN+FP+FN) 
* **Recall** : Recall, also known as sensitivity, it is the true positive rate (TPR). This measures the proportion of actual positive values that are correctly identified. Higher recall means fewer false negative values. 

                                    Recall = TP/ (TP+FN)



 
* **Precision** : Precision (positive predictive value) represents the degree to which repeated measurements under unchanged conditions show the same results.
                                    
                                    Precision = TP / (TP+FP)

* **F1 Score** : F1 score, also known as balanced F-score or F-measure, is the harmonic average of recall and precision, considering they have equal weight. This is a good metric when recall and precision cause conflict and hard to decide which one is better.

                F1 score = 2*recall*precision / (recall + precision)
    



### Detection Results:-![image](https://user-images.githubusercontent.com/69844239/127860197-ba1a5eb8-9746-46c7-9de8-edccd10ce881.png)





